{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647e7e58-297e-420f-a244-990d34be7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import glob\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341556b-3dd8-44bf-9bb6-dc11451afa69",
   "metadata": {},
   "source": [
    "### Hyper-paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76bc5bb-b3a6-4808-ac00-c3a291459224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n",
      "Compose(\n",
      "    Resize(size=[224, 224], interpolation=bilinear)\n",
      "    RandomRotation(degrees=[-45.0, 45.0], interpolation=nearest, expand=False, fill=0)\n",
      "    RandomResizedCrop(size=[224, 224], scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
      "    GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0))\n",
      ")\n",
      "2023-05-24 11:45:29,805 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mming686\u001b[0m (\u001b[33mdeeplearning-med\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DL_MED/Project/wandb/run-20230524_114531-ys4ugojp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeplearning-med/Unet/runs/ys4ugojp' target=\"_blank\">silvery-puddle-2</a></strong> to <a href='https://wandb.ai/deeplearning-med/Unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeplearning-med/Unet' target=\"_blank\">https://wandb.ai/deeplearning-med/Unet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeplearning-med/Unet/runs/ys4ugojp' target=\"_blank\">https://wandb.ai/deeplearning-med/Unet/runs/ys4ugojp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "np.random.seed(1024)\n",
    "device = torch.device(\n",
    "    \"cuda:1\"\n",
    "    if torch.cuda.is_available()\n",
    "    else\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([224, 224]),\n",
    "    transforms.GaussianBlur(3)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([224, 224])\n",
    "])\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "weight_decay = 0\n",
    "num_epochs = 20\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"Unet\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"Epoches number\": num_epochs,\n",
    "        \"transform\": str(transform),\n",
    "        \"target transform\": str(target_transform)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfe9b4-3b82-46c2-bb40-a7d8a900298d",
   "metadata": {},
   "source": [
    "### Create Segmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f99d828-b7b4-47a4-816b-81c3f9411538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, data_root, transform, target_transform, train=True):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        self.gt_files_path = []\n",
    "        # find all patient directories\n",
    "        patient_directories = glob.glob(os.path.join(self.data_root, 'patient*'))\n",
    "        # find all files with the suffix _gt.npy\n",
    "        for patient_directory in patient_directories:\n",
    "            per_patient_file_path = glob.glob(os.path.join(patient_directory, '*_gt.npy'))\n",
    "            for path in per_patient_file_path:\n",
    "                self.gt_files_path.append(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.gt_files_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        gt_image_path = self.gt_files_path[index]\n",
    "        image_path = gt_image_path[:-7] + \".npy\"\n",
    "        image = np.load(image_path)\n",
    "        gt_image = np.load(gt_image_path)\n",
    "        image = torch.tensor(image[None,:,:]).float()\n",
    "        gt_image = torch.tensor(gt_image).long()\n",
    "            \n",
    "        # Convert the ground truth label to one-hot encoding\n",
    "        one_hot_label = torch.nn.functional.one_hot(gt_image, num_classes=4)\n",
    "\n",
    "        # Transpose the tensor to have dimensions (C, H, W)\n",
    "        one_hot_label = one_hot_label.permute(2, 0, 1)\n",
    "\n",
    "        # Remove the background channel (dimension 0)\n",
    "        one_hot_label = one_hot_label[1:, :, :]\n",
    "        \n",
    "        # Use seed to make sure image and target has same transform\n",
    "        seed = np.random.randint(2147483647)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.transform(image)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        target = self.target_transform(one_hot_label)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29be4b64-a02f-4bde-9c06-249d4448f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SegDataset(data_root = './database/training', \n",
    "                     transform = transform, \n",
    "                     target_transform = target_transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a235bf-de5f-412c-bfd0-092df2a89025",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a5c018-ef5a-40e9-b7ac-5e38427fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels\n",
    "    classes=3,                      # model output channels (number of classes)\n",
    ")\n",
    "\n",
    "preprocess_input = get_preprocessing_fn('resnet50', pretrained='imagenet')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bbbbc-97b5-4db4-abfa-f8fafd42cf51",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459ad7a-ff4e-4f71-81eb-5947426b6672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/238 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 238/238 [01:22<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Loss: 1.2420823460892787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [01:23<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Loss: 0.7412007285069816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 237/238 [01:24<00:00,  2.81it/s]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "losses = []\n",
    "best_loss = 1e10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for step, (img, gt) in enumerate(tqdm(dataloader)):\n",
    "        img = img.to(device)\n",
    "        mask = model(img)\n",
    "        loss = seg_loss(mask, gt.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= step\n",
    "    losses.append(epoch_loss)\n",
    "    wandb.log({\"loss\": epoch_loss})\n",
    "    print(f'EPOCH: {epoch}, Loss: {epoch_loss}')\n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), './model/unet-test/model_best.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
