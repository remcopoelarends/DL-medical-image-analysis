{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "647e7e58-297e-420f-a244-990d34be7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "import glob\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341556b-3dd8-44bf-9bb6-dc11451afa69",
   "metadata": {},
   "source": [
    "### Set-up transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76bc5bb-b3a6-4808-ac00-c3a291459224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "np.random.seed(1024)\n",
    "device = torch.device(\n",
    "    \"cuda:1\"\n",
    "    if torch.cuda.is_available()\n",
    "    else\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([256, 256]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([256, 256]),\n",
    "    transforms.GaussianBlur(3)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize([256, 256]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([256, 256])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfe9b4-3b82-46c2-bb40-a7d8a900298d",
   "metadata": {},
   "source": [
    "### Create Segmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f99d828-b7b4-47a4-816b-81c3f9411538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, data_root, transform, target_transform, train=True):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        self.gt_files_path = []\n",
    "        # find all patient directories\n",
    "        patient_directories = glob.glob(os.path.join(self.data_root, 'patient*'))\n",
    "        # find all files with the suffix _gt.npy\n",
    "        train_size = int(len(patient_directories)*0.8)\n",
    "        \n",
    "        if self.train:\n",
    "            for patient_directory in patient_directories[0:train_size]:\n",
    "                per_patient_file_path = glob.glob(os.path.join(patient_directory, '*_gt.npy'))\n",
    "                for path in per_patient_file_path:\n",
    "                    self.gt_files_path.append(path)\n",
    "        else:\n",
    "            for patient_directory in patient_directories[train_size:]:\n",
    "                per_patient_file_path = glob.glob(os.path.join(patient_directory, '*_gt.npy'))\n",
    "                for path in per_patient_file_path:\n",
    "                    self.gt_files_path.append(path)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.gt_files_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        gt_image_path = self.gt_files_path[index]\n",
    "        image_path = gt_image_path[:-7] + \".npy\"\n",
    "        image = np.load(image_path)\n",
    "        gt_image = np.load(gt_image_path)\n",
    "        image = torch.tensor(image[None,:,:]).float()\n",
    "        gt_image = torch.tensor(gt_image).long()\n",
    "            \n",
    "        # Convert the ground truth label to one-hot encoding\n",
    "        one_hot_label = torch.nn.functional.one_hot(gt_image, num_classes=4)\n",
    "\n",
    "        # Transpose the tensor to have dimensions (C, H, W)\n",
    "        one_hot_label = one_hot_label.permute(2, 0, 1)\n",
    "\n",
    "        # Remove the background channel (dimension 0)\n",
    "        one_hot_label = one_hot_label[1:, :, :]\n",
    "        \n",
    "        # Use seed to make sure image and target has same transform\n",
    "        seed = np.random.randint(2147483647)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.transform(image)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        target = self.target_transform(one_hot_label)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29be4b64-a02f-4bde-9c06-249d4448f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegDataset(data_root = './database/training', \n",
    "                     transform = transform, \n",
    "                     target_transform = target_transform,\n",
    "                     train = True)\n",
    "\n",
    "val_dataset = SegDataset(data_root = './database/training', \n",
    "                     transform = transform, \n",
    "                     target_transform = target_transform,\n",
    "                     train = False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b9a53-5df7-4dfb-abfa-1339f4810b60",
   "metadata": {},
   "source": [
    "### Init hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a86af4-4ec3-4f3d-aa69-2870690ffc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vzhr2032) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/rpoelarends/Unet/runs/vzhr2032' target=\"_blank\">https://wandb.ai/rpoelarends/Unet/runs/vzhr2032</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230530_102606-vzhr2032/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vzhr2032). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7b3d7ca84046e78309ba4935337b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669822418286153, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DL_for_medical_imaging_analysis/Final_Project/wandb/run-20230530_102754-waxrlsje</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rpoelarends/Unet/runs/waxrlsje' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/rpoelarends/Unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rpoelarends/Unet' target=\"_blank\">https://wandb.ai/rpoelarends/Unet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rpoelarends/Unet/runs/waxrlsje' target=\"_blank\">https://wandb.ai/rpoelarends/Unet/runs/waxrlsje</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "weight_decay = 0\n",
    "num_epochs = 10\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"Unet\",\n",
    "    name='test',\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"Epoches number\": num_epochs,\n",
    "        \"transform\": str(transform),\n",
    "        \"target transform\": str(target_transform)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a235bf-de5f-412c-bfd0-092df2a89025",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a5c018-ef5a-40e9-b7ac-5e38427fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels\n",
    "    classes=3,                      # model output channels (number of classes)\n",
    ")\n",
    "\n",
    "#preprocess_input = get_preprocessing_fn('resnet50', pretrained='imagenet')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bbbbc-97b5-4db4-abfa-f8fafd42cf51",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0650ddcb-f6f0-4ce1-8034-fabceca1e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_img(img, mask):\n",
    "    # img: (B, 256, 64, 64), {: (B, 1, 256, 256)\n",
    "    print(f\"{img.shape=}, {mask.shape=}\")\n",
    "    img = np.squeeze(img)\n",
    "    mask = np.squeeze(mask)\n",
    "    plt.figure()\n",
    "    plt.imshow(img, 'gray')\n",
    "    overlay_mask_0 = np.ma.masked_where(mask[0] == 0, img)\n",
    "    overlay_mask_1 = np.ma.masked_where(mask[1] == 0, img)\n",
    "    overlay_mask_2 = np.ma.masked_where(mask[2] == 0, img)\n",
    "    plt.imshow(overlay_mask_0, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    plt.imshow(overlay_mask_1, 'Reds', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    plt.imshow(overlay_mask_2, 'Purples', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='jpeg')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Convert the in-memory buffer to a NumPy array\n",
    "    image_array = np.array(Image.open(buffer))\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459ad7a-ff4e-4f71-81eb-5947426b6672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:55<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Train Loss: 51.06673422455788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Validation Loss: 11.386808454990387\n",
      "img.shape=(256, 256), mask.shape=(3, 256, 256)\n",
      "img.shape=(256, 256), mask.shape=(3, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:57<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2, Train Loss: 41.43707627058029\n",
      "EPOCH: 2, Validation Loss: 11.386808454990387\n",
      "img.shape=(256, 256), mask.shape=(3, 256, 256)\n",
      "img.shape=(256, 256), mask.shape=(3, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:59<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3, Train Loss: 35.30691337585449\n",
      "EPOCH: 3, Validation Loss: 11.386808454990387\n",
      "img.shape=(256, 256), mask.shape=(3, 256, 256)\n",
      "img.shape=(256, 256), mask.shape=(3, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 80/96 [00:48<00:12,  1.33it/s]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "best_loss = 1e10\n",
    "val_freq = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, (img, gt) in enumerate(tqdm(train_loader)):\n",
    "        img = img.to(device)\n",
    "        mask = model(img)\n",
    "        loss = seg_loss(mask, gt.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'EPOCH: {epoch + 1}, Train Loss: {epoch_loss}')\n",
    "    \n",
    "    # Validation\n",
    "    if epoch % val_freq == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        last_image_batch = None\n",
    "        last_gt_mask_batch = None\n",
    "        last_pr_mask_batch = None\n",
    "        with torch.no_grad():\n",
    "            for step, (img, gt) in enumerate(tqdm(val_loader)):\n",
    "                img = img.to(device)\n",
    "                mask = model(img)\n",
    "                loss = seg_loss(mask, gt.to(device))\n",
    "                val_loss += loss.item()\n",
    "                last_image_batch = img\n",
    "                last_gt_mask_batch = gt\n",
    "                last_pr_mask_batch = mask\n",
    "            \n",
    "    print(f'EPOCH: {epoch + 1}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    last_image = last_image_batch.detach().cpu().numpy()[0][0]\n",
    "    last_gt = last_gt_mask_batch.detach().cpu().numpy()[0]\n",
    "    last_pr = last_pr_mask_batch.detach().cpu().numpy()[0]\n",
    "    \n",
    "    threshold = 0.95  # Set your desired threshold value\n",
    "    binary_mask = (last_pr > threshold)\n",
    "    \n",
    "    ground_truth = vis_img(last_image, last_gt)\n",
    "    predicted = vis_img(last_image, binary_mask)\n",
    "    # Log\n",
    "    wandb.log({\"loss\": epoch_loss,\n",
    "               \"val_loss\": val_loss,\n",
    "               \"ground_truth\": wandb.Image(ground_truth),\n",
    "               \"prediction\": wandb.Image(predicted)})\n",
    "    \n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), './model/unet-test/model_best.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
