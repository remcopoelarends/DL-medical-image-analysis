{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32858f7c-7a23-4503-88f0-a4100a5ba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import optuna\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "import glob\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a8ed05-fb81-4038-a4f4-59748002c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "np.random.seed(1024)\n",
    "device = torch.device(\n",
    "    \"cuda:1\"\n",
    "    if torch.cuda.is_available()\n",
    "    else\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.45, ], [0.35, ]),\n",
    "    transforms.Resize([256, 256]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([256, 256])\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize([256, 256], interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.RandomRotation(45, interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.RandomResizedCrop([256, 256], interpolation=transforms.InterpolationMode.NEAREST)\n",
    "])\n",
    "\n",
    "# Validation Data Augmentation\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.45, ], [0.35, ]),\n",
    "    transforms.Resize([256, 256])\n",
    "])\n",
    "\n",
    "val_target_transform = transforms.Compose([\n",
    "    transforms.Resize([256, 256], interpolation=transforms.InterpolationMode.NEAREST)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7494ecc2-b4ae-4004-bb9d-c0460f9f39c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, data_root, transform, target_transform, \n",
    "                 include_background = True, train=True, to3d=False):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        self.to3d = to3d\n",
    "        self.gt_files_path = []\n",
    "        self.include_background = include_background\n",
    "        \n",
    "        # find all patient directories\n",
    "        patient_directories = glob.glob(os.path.join(self.data_root, 'patient*'))\n",
    "        # find all files with the suffix _gt.npy\n",
    "        train_size = int(len(patient_directories)*0.8)\n",
    "        \n",
    "        if self.train:\n",
    "            for patient_directory in patient_directories[0:train_size]:\n",
    "                per_patient_file_path = glob.glob(os.path.join(patient_directory, '*_gt.npy'))\n",
    "                for path in per_patient_file_path:\n",
    "                    self.gt_files_path.append(path)\n",
    "        else:\n",
    "            for patient_directory in patient_directories[train_size:]:\n",
    "                per_patient_file_path = glob.glob(os.path.join(patient_directory, '*_gt.npy'))\n",
    "                for path in per_patient_file_path:\n",
    "                    self.gt_files_path.append(path)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.gt_files_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        gt_image_path = self.gt_files_path[index]\n",
    "        image_path = gt_image_path[:-7] + \".npy\"\n",
    "        image = np.load(image_path)\n",
    "        gt_image = np.load(gt_image_path)\n",
    "        image = torch.tensor(image[None,:,:]).float()\n",
    "        gt_image = torch.tensor(gt_image).long()\n",
    "        # Convert the ground truth label to one-hot encoding\n",
    "        one_hot_label = torch.nn.functional.one_hot(gt_image, num_classes=4)\n",
    "\n",
    "        # Transpose the tensor to have dimensions (C, H, W)\n",
    "        one_hot_label = one_hot_label.permute(2, 0, 1)\n",
    "        \n",
    "        if not self.include_background:\n",
    "            # Remove the background channel (dimension 0)\n",
    "            one_hot_label = one_hot_label[1:, :, :]\n",
    "        \n",
    "        # Use seed to make sure image and target has same transform\n",
    "        seed = np.random.randint(2147483647)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        target = self.target_transform(one_hot_label)\n",
    "        \n",
    "        # For 3 dimension input channel\n",
    "        if self.to3d:\n",
    "            rgb_tensor = image.repeat(3, 1, 1)\n",
    "            return rgb_tensor, target\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4720964e-c7a4-4e6a-bf1a-776d76c01775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegDataset(data_root='./database/training',\n",
    "                           transform=transform,\n",
    "                           target_transform=target_transform,\n",
    "                           train=True,\n",
    "                           to3d=False)\n",
    "\n",
    "val_dataset = SegDataset(data_root='./database/training',\n",
    "                         transform=val_transform,\n",
    "                         target_transform=val_target_transform,\n",
    "                         train=False,\n",
    "                         to3d=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52dcbd8-2438-486f-9217-2498e0e66c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_img(img, mask):\n",
    "    img = np.squeeze(img)\n",
    "    mask = np.squeeze(mask)\n",
    "    plt.figure()\n",
    "    plt.imshow(img, 'gray')\n",
    "    overlay_mask_1 = np.ma.masked_where(mask[1] == 0, img)\n",
    "    overlay_mask_2 = np.ma.masked_where(mask[2] == 0, img)\n",
    "    overlay_mask_3 = np.ma.masked_where(mask[3] == 0, img)\n",
    "    plt.imshow(overlay_mask_1, 'Greens', alpha=1, interpolation='nearest')\n",
    "    plt.imshow(overlay_mask_2, 'Reds', alpha=1, interpolation='nearest')\n",
    "    plt.imshow(overlay_mask_3, 'Purples', alpha=1, interpolation='nearest')\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='jpeg')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Convert the in-memory buffer to a NumPy array\n",
    "    image_array = np.array(Image.open(buffer))\n",
    "    plt.close()\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e138255-68b0-4ab0-b02d-db7369c915ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trial):\n",
    "\n",
    "    # Hyperparameters to be optimized\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n",
    "    batch_size = 8\n",
    "    weight_decay = 1e-5\n",
    "    optimizer_name = \"Adam\"\n",
    "    loss_func = trial.suggest_categorical(\"loss_func\",\n",
    "                                          [\"DiceLoss\",\n",
    "                                           \"GeneralizedDiceLoss\",\n",
    "                                           \"TverskyLoss\"])\n",
    "\n",
    "    # Choose loss function\n",
    "    if loss_func == \"DiceLoss\":\n",
    "        seg_loss = monai.losses.DiceLoss(sigmoid=True,\n",
    "                                         squared_pred=True,\n",
    "                                         reduction='mean')\n",
    "    elif loss_func == \"GeneralizedDiceLoss\":\n",
    "        seg_loss = monai.losses.GeneralizedDiceLoss(sigmoid=True)\n",
    "    elif loss_func == \"TverskyLoss\":\n",
    "        seg_loss = monai.losses.TverskyLoss(sigmoid=True)\n",
    "    \n",
    "    num = trial.number + 21\n",
    "    run = wandb.init(\n",
    "        project=\"Unet-Res50-Tune\",\n",
    "        name=f'Unet-Res50-trial-{num}',\n",
    "        config={\n",
    "            \"number of epoches\": 25,\n",
    "            \"learning rate\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"weight decay\": weight_decay,\n",
    "            \"loss function\": loss_func,\n",
    "            \"transform\": str(transform),\n",
    "            \"target transform\": str(target_transform)\n",
    "        })\n",
    "\n",
    "    # Set up DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Set up model and optimizer\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet50\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=1,\n",
    "        classes=4\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                     lr=lr,\n",
    "                                     weight_decay=weight_decay)\n",
    "\n",
    "    best_loss = 1e10\n",
    "    # Start training and validation\n",
    "    for epoch in range(20):\n",
    "        # Train\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for step, (img, gt) in enumerate(tqdm(train_loader)):\n",
    "            img = img.to(device)\n",
    "            mask = model(img)\n",
    "            loss = seg_loss(mask, gt.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f'EPOCH: {epoch + 1}, Train Loss: {epoch_loss}')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        last_image_batch = None\n",
    "        last_gt_mask_batch = None\n",
    "        last_pr_mask_batch = None\n",
    "        with torch.no_grad():\n",
    "            for step, (img, gt) in enumerate(tqdm(val_loader)):\n",
    "                img = img.to(device)\n",
    "                mask = model(img)\n",
    "                loss = seg_loss(mask, gt.to(device))\n",
    "                val_loss += loss.item()\n",
    "                last_image_batch = img\n",
    "                last_gt_mask_batch = gt\n",
    "                last_pr_mask_batch = mask\n",
    "\n",
    "        print(f'EPOCH: {epoch + 1}, Validation Loss: {val_loss}')\n",
    "\n",
    "        last_image = last_image_batch.detach().cpu().numpy()[0][0]\n",
    "        last_gt = last_gt_mask_batch.detach().cpu().numpy()[0]\n",
    "        last_pr = last_pr_mask_batch.detach().cpu().numpy()[0]\n",
    "\n",
    "        threshold = 0.95\n",
    "        binary_mask = (last_pr > threshold)\n",
    "\n",
    "        ground_truth = vis_img(last_image, last_gt)\n",
    "        predicted = vis_img(last_image, binary_mask)\n",
    "        # Log\n",
    "        wandb.log({\"loss\": epoch_loss,\n",
    "                   \"val_loss\": val_loss,\n",
    "                   \"ground_truth\": wandb.Image(ground_truth),\n",
    "                   \"prediction\": wandb.Image(predicted)})\n",
    "\n",
    "        # save the best model\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), f'./model/unet-tune/model_best_{trial.number}.pth')\n",
    "\n",
    "    # Return the validation loss as this is the value to be optimized\n",
    "    run.finish()\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23afbaa1-3754-4435-8dd9-f2b9521de13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-16 12:29:11,722] A new study created in memory with name: no-name-1c0547d5-ea7e-40d1-81f2-d0081e53a36c\n",
      "/tmp/ipykernel_3336/2982568370.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-16 12:29:12,047 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mming686\u001b[0m (\u001b[33mdeeplearning-med\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DL_MED/Project/wandb/run-20230616_122913-bo5jl0z1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeplearning-med/Unet-Res50-Tune/runs/bo5jl0z1' target=\"_blank\">Unet-Res50-trial-21</a></strong> to <a href='https://wandb.ai/deeplearning-med/Unet-Res50-Tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeplearning-med/Unet-Res50-Tune' target=\"_blank\">https://wandb.ai/deeplearning-med/Unet-Res50-Tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeplearning-med/Unet-Res50-Tune/runs/bo5jl0z1' target=\"_blank\">https://wandb.ai/deeplearning-med/Unet-Res50-Tune/runs/bo5jl0z1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 7/192 [00:03<01:37,  1.90it/s]\n",
      "[W 2023-06-16 12:29:22,507] Trial 0 failed with parameters: {'lr': 0.00031063343020566443, 'loss_func': 'TverskyLoss'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3336/2982568370.py\", line 69, in train_model\n",
      "    optimizer.step()\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/torch/optim/adam.py\", line 108, in step\n",
      "    F.adam(params_with_grad,\n",
      "  File \"/home/jovyan/.local/lib/python3.8/site-packages/torch/optim/_functional.py\", line 85, in adam\n",
      "    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
      "KeyboardInterrupt\n",
      "[W 2023-06-16 12:29:22,531] Trial 0 failed with value None.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(train_model, n_trials=20)\n",
    "\n",
    "# Print out the best hyperparameters\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774544a9-efcc-4ed1-afad-d9e467eb25cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
