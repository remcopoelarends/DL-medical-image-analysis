{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647e7e58-297e-420f-a244-990d34be7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "import wandb\n",
    "import glob\n",
    "import torch\n",
    "import monai\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341556b-3dd8-44bf-9bb6-dc11451afa69",
   "metadata": {},
   "source": [
    "### Hyper-paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76bc5bb-b3a6-4808-ac00-c3a291459224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1 device\n",
      "2023-05-26 16:12:28,352 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mming686\u001b[0m (\u001b[33mdeeplearning-med\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/DL_MED/Project/wandb/run-20230526_161230-cspy6l83</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeplearning-med/Unet/runs/cspy6l83' target=\"_blank\">easy-bird-8</a></strong> to <a href='https://wandb.ai/deeplearning-med/Unet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeplearning-med/Unet' target=\"_blank\">https://wandb.ai/deeplearning-med/Unet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeplearning-med/Unet/runs/cspy6l83' target=\"_blank\">https://wandb.ai/deeplearning-med/Unet/runs/cspy6l83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "np.random.seed(1024)\n",
    "device = torch.device(\n",
    "    \"cuda:1\"\n",
    "    if torch.cuda.is_available()\n",
    "    else\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([224, 224]),\n",
    "    transforms.GaussianBlur(3)\n",
    "])\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomResizedCrop([224, 224])\n",
    "])\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "weight_decay = 0\n",
    "num_epochs = 20\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"Unet\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"weight decay\": weight_decay,\n",
    "        \"Epoches number\": num_epochs,\n",
    "        \"transform\": str(transform),\n",
    "        \"target transform\": str(target_transform)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bfe9b4-3b82-46c2-bb40-a7d8a900298d",
   "metadata": {},
   "source": [
    "### Create Segmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f99d828-b7b4-47a4-816b-81c3f9411538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, data_root, transform, target_transform, train=True):\n",
    "        self.data_root = data_root\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train\n",
    "        self.gt_files_path = []\n",
    "        # find all patient directories\n",
    "        patient_directories = glob.glob(os.path.join(self.data_root, 'patient*'))\n",
    "        # find all files with the suffix _gt.npy\n",
    "        for patient_directory in patient_directories:\n",
    "            per_patient_file_path = glob.glob(os.path.join(patient_directory, '*_gt.npy'))\n",
    "            for path in per_patient_file_path:\n",
    "                self.gt_files_path.append(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.gt_files_path)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        gt_image_path = self.gt_files_path[index]\n",
    "        image_path = gt_image_path[:-7] + \".npy\"\n",
    "        image = np.load(image_path)\n",
    "        gt_image = np.load(gt_image_path)\n",
    "        image = torch.tensor(image[None,:,:]).float()\n",
    "        gt_image = torch.tensor(gt_image).long()\n",
    "            \n",
    "        # Convert the ground truth label to one-hot encoding\n",
    "        one_hot_label = torch.nn.functional.one_hot(gt_image, num_classes=4)\n",
    "\n",
    "        # Transpose the tensor to have dimensions (C, H, W)\n",
    "        one_hot_label = one_hot_label.permute(2, 0, 1)\n",
    "\n",
    "        # Remove the background channel (dimension 0)\n",
    "        one_hot_label = one_hot_label[1:, :, :]\n",
    "        \n",
    "        # Use seed to make sure image and target has same transform\n",
    "        seed = np.random.randint(2147483647)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        image = self.transform(image)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        target = self.target_transform(one_hot_label)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29be4b64-a02f-4bde-9c06-249d4448f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SegDataset(data_root = './database/training', \n",
    "                     transform = transform, \n",
    "                     target_transform = target_transform)\n",
    "# Split into train set and validation set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a235bf-de5f-412c-bfd0-092df2a89025",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a5c018-ef5a-40e9-b7ac-5e38427fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=1,                  # model input channels\n",
    "    classes=3,                      # model output channels (number of classes)\n",
    ")\n",
    "\n",
    "preprocess_input = get_preprocessing_fn('resnet50', pretrained='imagenet')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bbbbc-97b5-4db4-abfa-f8fafd42cf51",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd63c44-18b1-4e8f-bc48-8baec987cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_img(img, mask):\n",
    "    # img: (B, 256, 64, 64), {: (B, 1, 256, 256)\n",
    "    print(f\"{img.shape=}, {mask.shape=}\")\n",
    "    img = np.squeeze(img)\n",
    "    mask = np.squeeze(mask)\n",
    "    plt.figure()\n",
    "    plt.imshow(img, 'gray')\n",
    "    overlay_mask_0 = np.ma.masked_where(mask[0] == 0, img)\n",
    "    overlay_mask_1 = np.ma.masked_where(mask[1] == 0, img)\n",
    "    overlay_mask_2 = np.ma.masked_where(mask[2] == 0, img)\n",
    "    plt.imshow(overlay_mask_0, 'Greens', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    plt.imshow(overlay_mask_1, 'Reds', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    plt.imshow(overlay_mask_2, 'Purples', alpha = 0.7, clim=[0,1], interpolation='nearest')\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format='jpeg')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Convert the in-memory buffer to a NumPy array\n",
    "    image_array = np.array(Image.open(buffer))\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459ad7a-ff4e-4f71-81eb-5947426b6672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:41<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Train Loss: 1.1762848788186124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Validation Loss: 0.837933379284879\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:42<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2, Train Loss: 0.7137712481774782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2, Validation Loss: 0.6506437633899932\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:42<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3, Train Loss: 0.586770035404908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3, Validation Loss: 0.5767067639117546\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:41<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4, Train Loss: 0.5431366805967531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4, Validation Loss: 0.5536772925803002\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:43<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, Train Loss: 0.5272362749827536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5, Validation Loss: 0.5347043351924166\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:39<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6, Train Loss: 0.5144824864048707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6, Validation Loss: 0.5370964591807508\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:42<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7, Train Loss: 0.495151686197833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7, Validation Loss: 0.5042009486797008\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:41<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8, Train Loss: 0.4948288541091116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8, Validation Loss: 0.5173850465328136\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:42<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9, Train Loss: 0.48874271072839437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9, Validation Loss: 0.5027152471085812\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:43<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10, Train Loss: 0.4872175522540745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10, Validation Loss: 0.5199826543635511\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:41<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11, Train Loss: 0.4823729047649785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.03it/s]\n",
      "/tmp/ipykernel_2683/3039285671.py:6: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 11, Validation Loss: 0.500339666579632\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      "100%|██████████| 191/191 [00:42<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12, Train Loss: 0.48246732479647586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 12, Validation Loss: 0.5030604239473951\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n",
      "img.shape=(224, 224), mask.shape=(3, 224, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/191 [00:00<?, ?it/s]/home/jovyan/.local/lib/python3.8/site-packages/monai/losses/dice.py:708: UserWarning: Multichannel targets are not supported in this older Pytorch version 1.8.0+cu111. Using argmax (as a workaround) to convert target to a single channel.\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 33/191 [00:07<00:36,  4.29it/s]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "best_loss = 1e10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, (img, gt) in enumerate(tqdm(train_loader)):\n",
    "        img = img.to(device)\n",
    "        mask = model(img)\n",
    "        loss = seg_loss(mask, gt.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= step\n",
    "    print(f'EPOCH: {epoch + 1}, Train Loss: {epoch_loss}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    last_image_batch = None\n",
    "    last_gt_mask_batch = None\n",
    "    last_pr_mask_batch = None\n",
    "    with torch.no_grad():\n",
    "        for step, (img, gt) in enumerate(tqdm(val_loader)):\n",
    "            img = img.to(device)\n",
    "            mask = model(img)\n",
    "            loss = seg_loss(mask, gt.to(device))\n",
    "            val_loss += loss.item()\n",
    "            last_image_batch = img\n",
    "            last_gt_mask_batch = gt\n",
    "            last_pr_mask_batch = mask\n",
    "            \n",
    "    val_loss /= step\n",
    "    print(f'EPOCH: {epoch + 1}, Validation Loss: {val_loss}')\n",
    "    \n",
    "    last_image = last_image_batch.detach().cpu().numpy()[0][0]\n",
    "    last_gt = last_gt_mask_batch.detach().cpu().numpy()[0]\n",
    "    last_pr = last_pr_mask_batch.detach().cpu().numpy()[0]\n",
    "    \n",
    "    threshold = 0.95  # Set your desired threshold value\n",
    "    binary_mask = (last_pr > threshold)\n",
    "    \n",
    "    ground_truth = vis_img(last_image, last_gt)\n",
    "    predicted = vis_img(last_image, binary_mask)\n",
    "    # Log\n",
    "    wandb.log({\"loss\": epoch_loss,\n",
    "               \"val_loss\": val_loss,\n",
    "               \"ground_truth\": wandb.Image(ground_truth),\n",
    "               \"prediction\": wandb.Image(predicted)})\n",
    "    \n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), './model/unet-test/model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457e8aa-0d8c-47c8-b7c0-6fe2744e8c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
